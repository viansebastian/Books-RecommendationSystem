# -*- coding: utf-8 -*-
"""Sub2_RecommendationSystem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17XuxE1VuDDhHloS7757Ki810n0ZZIOdy

# ***Laporan Proyek Machine Learning: Books Recommendation System - Vian Sebastian Bromokusumo***


## ***Project Overview***

---

***Latar Belakang***

Menurut survei oleh Badan Pusat Statistik Indonesia yang dipublikasikan pada 7 Juni 2023, jumlah populasi Indonesia adalah sebesar 278,696 juta [[1]](https://www.bps.go.id/id/statistics-table/2/MTk3NSMy/jumlah-penduduk-pertengahan-tahun--ribu-jiwa-.html). Namun sangat disayangkan, menurut survei UNESCO [[2]](https://uis.unesco.org/en/country/id), hanya sekitar 0.001% dari masyarakat Indonesia yang memiliki minat membaca. Hal ini tentunya sangat berpengaruh terhadap tingkat literasi Indonesia, dimana menurut Balai Bahasa Provinsi Sumatera Utara [[3]](https://balaibahasasumut.kemdikbud.go.id/2023/09/07/manca-untuk-literasi-yang-menyenangkan/#:~:text=Dengan%20kata%20lain%2C%20Indonesia%20masuk,2022%20mencapai%2051%2C69%25.), Indonesia masuk dalam 10 negara dengan tingkat literasi terendah. Maka dari itu, diperlukan strategi yang efektif dan efisien untuk meningkatkan minta membaca masyarakat Indonesia, dengan harapan meningkatkan tingkat literasi secara keseluruhan.

Proyek ini difokuskan pada dataset "Book Recommendation Dataset" dengan tujuan untuk menghasilkan suatu Recommendation System yang dapat merekomendasikan buku yang relevan kepada user. Dalam implementasi lanjutannya, harapannya Recommendation System ini dapat memberikan rekomendasi mulai dari literasi ilmiah, buku-buku hiburan, hingga barang-barang dalam bisnis e-commerce dan film-film dalam bisnis perfilman. Jumlah data yang sangat besar dalam dataset ini menjadi salah satu alasan Penulis menggunakan dataset ini, dikarenakan dataset yang besar akan membantu model untuk membangun sistem rekomendasi yang lebih baik dan custom bagi user.  

Menurut Jepchumba dari Microsoft [[4]](https://techcommunity.microsoft.com/t5/educator-developer-blog/getting-started-with-using-visual-machine-learning-tools-for/ba-p/3578397), *machine learning* merupakan teknik yang menggunakan matematika tingkat tinggi dan ilmu statistika untuk mengenali pola pada data yang tidak ada secara eksplisit, dan dapat memprediksi sesuai dengan hasil pola tersebut. Terdapat dua metode yang banyak digunakan dalam domain sistem rekomendasi ini, antara lain Content Based Filtering, dan Collaborative Filtering.

Menurut Google Machine Learning Developers, Content Based Filtering [[5]](https://developers.google.com/machine-learning/recommendation/content-based/basics) merupakan teknik mendapatkan rekomendasi item berdasarkan items yang disukai user, dengan cara menghitung Cosine Similarity antar items. Hal ini tentu sangat berat dalam komputasinya, mengingat dalam kasus nyata, akan ada ribuan hingga jutaan items yang perlu dikaji untuk user. Ditambah lagi, menurut sumber yang sama, Collaborative Filtering [[6]](https://developers.google.com/machine-learning/recommendation/collaborative/basics) merupakan teknik yang mengkaji bukan hanya items yang disukai user, namun komunitas user itu sendiri untuk menghasilkan rekomendasi yang lebih baik. Hal ini tentunya membutuhkan komputasi yang lebih rumit lagi, menggunakan Deep Learning seperti Neural Network. Alhasil, dalam membangun Recommendation System, diperlukan Machine Learning untuk mempercepat dan menghasilkan rekomendasi yang efektif bagi user dan sasaran lainnya.

Proyek ini menjadi sarana kecil untuk membantu meningkatkan minat membaca dan literasi Indonesia, dan hasil dari proyek ini diharapkan dapat membantu Pemerintah, instansi literatur, hingga individual untuk mengembangkan minat membaca dan edukasi literasi.

## ***Business Understanding***

Stakeholder dan sasaran:

1. Pemerintah

  Sebagai organisasi tingkat tertinggi dalam negara, tentunya pemerintah dapat menggunakan Recommendation System untuk meningkatkan minta baca masyarakat. Dengan strategi lainnya yang dapat digunakan oleh pemerintah, sistem rekomendasi yang baik diharapkan dapat mendukung strategi pemerintah dalam meningkatkan minat baca dan literasi Indonesia.

2. Perpustakaan

  Sebagai instansi literatur paling tua di dunia, tentunya perpustakaan-perpustakaan menyimpan ilmu-ilmu yang sudah ada sejak lama. Recommendation System yang baik akan membantu perpustakaan untuk melayani pendatang dan pembaca yang datang untuk mencari berbagai referensi dan literatur lainnya dengan lebih baik.

3. Mesin Pencari

  Bukan hanya perpustakaan, Mesin Pencari seperti Google Scholar sudah menjadi gudang literasi penelitian bagi para ilmuwan dan pelajar yang ingin meningkatkan ilmunya dalam bidang tertentu. Recommendation System yang baik akan membantu pembaca untuk lebih mudah mencari informasi yang berhubungan, guna meningkatkan literasinya dalam ilmu yang didalami.

4. Individu

  Dalam konteks individu, tentunya Recommendation System tidak hanya berguna dalam konteks mencari ilmu, namun juga dapat berguna untuk membantu pembaca-pembaca mencari hiburan seperti buku fiksi dan genre-genre lainnya. Harapannya, output proyek ini dapat mendukung kemajuan literasi per individu.

**Problem Statement**

1. Dengan banyaknya jumlah buku dan genre, apakah Recommendation System yang tepat sasaran (efektif) dapat dibuat?

**Recommendation System Goals**
1. Membuat Recommendation System yang efektif dan tepat sasaran, guna membantu sasaran (perpustakaan, mesin pencari, maupun individu).

**Solution Statements (Metodologi)**
1.  Melakukan Exploratory Data Analysis untuk mendapatkan informasi berguna dalam data dan mengetahui dinamika fitur-fitur.

2. Membuat model machine learning yang dapat merekomendasikan buku dengan tepat sasaran, menggunakan metode Content Based Filtering dan Collaborative Filtering.

3. Menggunakan metrik evaluasi Precision@k dan Root Mean Squared Error untuk mengevaluasi performa model.

### ***Data Understanding***

---

Dataset: https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path


books = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dicoding Machine Learning Implementation/archive (4)/books/Books.csv')
ratings = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dicoding Machine Learning Implementation/archive (4)/books/Ratings.csv')
users = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dicoding Machine Learning Implementation/archive (4)/books/Users.csv')


print(f"books data : {books.shape}")
print(f"ratings data: {ratings.shape}")
print(f"users data: {users.shape}")

books

ratings

users

books.info()

ratings.info()

users.info()

books.isnull().sum()

ratings.isnull().sum()

users.isnull().sum()

"""### ***EDA - Univariate Analysis***

---
"""

# ratings dataframe
rating_users = len(ratings['User-ID'].unique())
rated_books = len(ratings['ISBN'].unique())
rating_values = ratings['Book-Rating'].unique()

print(f"rating users: {rating_users}")
print(f"rated books: {rated_books}")
print(f"rating range: {rating_values}")

# books dataframe
registered_books = len(books['ISBN'].unique())
authors_count = len(books['Book-Author'].unique())
titles_count = len(books['Book-Title'].unique())

print(f"registered books: {registered_books}")
print(f"registered authors: {authors_count}")
print(f"registered titles: {titles_count}")

# users dataframe
registered_users = len(users['User-ID'].unique())

print(f"registered users: {registered_users}")

"""*Beberapa hasil dari Univariate Analysis pada data ratings, users, dan books adalah sebagai berikut.*

---



1. dari total user yang terdaftar (278,853 users), hanya 105,283 user yang melakukan rating

2. dari total buku yang terdaftar (271,360 buku), terdapat 340,556 buku yang diberikan rating. Hal ini dapat disebabkan oleh buku yang tidak terdaftar pada data Books, namun ada di ratings.

3. terdapat 102,024 penulis yang terdaftar.

4. rating terdiri dari skala 1-10.

5. Terdapat perbedaan jumlah buku (ISBN) dan judul. Hal ini dapat disebabkan oleh judul yang sama, namun berbeda versi.

### ***Data Preparation***

---

1. Data Merging
2. Missing Values Handling
3. Duplicates Data Handling
4. Data Selection

***Data Merging***

---
"""

books_rate_data = pd.merge(
    ratings,
    books,
    on ='ISBN',
    how ='left'
)

books_rate_data

"""***Missing Values Handling***

---


"""

books_rate_data.isnull().sum()

books_rate_data = books_rate_data.dropna()
books_rate_data

books_rate_data = books_rate_data.sort_values('ISBN', ascending = True)
books_rate_data

"""***Duplicates Data Handling***

---


"""

# check for duplicates
duplicate_count_isbn = books_rate_data.duplicated(subset = ['ISBN']).sum()
duplicate_count_title = books_rate_data.duplicated(subset = ['Book-Title']).sum()

print(f"Number of duplicates in the 'ISBN' column: {duplicate_count_isbn}")
print(f"Number of duplicates in the 'Book-Title' column: {duplicate_count_title}")

books_rate_data = books_rate_data.drop_duplicates('ISBN')
books_rate_data = books_rate_data.drop_duplicates('Book-Title')

books_rate_data

duplicate_count_isbn = books_rate_data.duplicated(subset = ['ISBN']).sum()
duplicate_count_title = books_rate_data.duplicated(subset = ['Book-Title']).sum()

print(f"Number of duplicates in the 'ISBN' column: {duplicate_count_isbn}")
print(f"Number of duplicates in the 'Book-Title' column: {duplicate_count_title}")

books_rate_data.isnull().sum()

"""***Data Selection***

---


"""

# Data yang digunakan hanya 30,000 sampel, dikarenakan RAM yang kecil dari Google Collab gratis
books_rate_data = books_rate_data.iloc[:30000,:]

isbn = books_rate_data['ISBN'].tolist()
authors = books_rate_data['Book-Author'].tolist()
titles = books_rate_data['Book-Title'].tolist()

print(len(isbn))
print(len(authors))
print(len(titles))

books_new = pd.DataFrame({
    'ISBN': isbn,
    'Authors': authors,
    'Titles': titles

})

books_new

"""### ***Data Preparation - Collaborative Filtering***

---
1. Encoding and Mapping
2. Fetching Random Samples
3. Train Test Split

"""

df = books_rate_data
df

"""***Encoding and Mapping***

---


"""

user_id = df['User-ID'].unique().tolist()
encode_user_id1 = {x: i for i, x in enumerate(user_id)}
encoded_user_id2 = {i: x for i, x in enumerate(user_id)}

print(f"user ID's : {user_id}")
print(f"encoded user: {encode_user_id1}")
print(f"encoded user ID's: {encoded_user_id2}")

title = df['Book-Title'].unique().tolist()
encode_title1 = {x: i for i, x in enumerate(title)}
encoded_title2 = {i: x for i, x in enumerate(title)}

print(f"resto's : {title}")
print(f"encoded resto: {encode_title1}")
print(f"encoded resto's: {encoded_title2}")

df['user'] = df['User-ID'].map(encode_user_id1)
df['books'] = df['Book-Title'].map(encode_title1)

df

num_users = len(df['user'].unique())
num_books = len(df['books'].unique())
min_rating = min(df['Book-Rating'])
max_rating = max(df['Book-Rating'])

print(f"number of users: {num_users}")
print(f"number of books: {num_books}")
print(f"min rating: {min_rating}")
print(f"max rating: {max_rating}")

"""***Fetching Random Samples***

---


"""

df = df.sample(frac = 1, random_state = 123)
df

"""***Train Test Split***

---


"""

x = df[['user', 'books']].values
y = df['Book-Rating'].apply(lambda x: (x - min_rating)/ (max_rating - min_rating))

train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

"""### ***Model Development - Content Based Filtering***

---
1. Vectorizer Calculations
2. Cosine Similarity Calculations
3. Recommendation Retrieval Function Initalization
4. Sample Retrieval
5. Recommendation Result -- Top-N Recommendation

"""

data = books_new
data

"""***Vectorizer Calculations***

---


"""

from sklearn.feature_extraction.text import TfidfVectorizer

vect = TfidfVectorizer()

vect.fit(data['Titles'])
vect.get_feature_names_out()

tfidf_matrix = vect.fit_transform(data['Titles'])

tfidf_matrix.shape

print(tfidf_matrix)

a = tfidf_matrix.todense()

pd.DataFrame(
    # tfidf_matrix.todense(),
    a,
    columns = vect.get_feature_names_out(),
    index = data.Titles
).sample(22, axis = 1).sample(10, axis = 0)

"""***Cosine Similarity Calculations***

---


"""

from sklearn.metrics.pairwise import cosine_similarity

sim = cosine_similarity(tfidf_matrix)
sim

sim_df = pd.DataFrame(
    sim,
    index = data['Titles'],
    columns = data['Titles']
)

sim_df.sample(5, axis = 1).sample(10, axis = 0)

"""***Recommendation Retrieval Function Initialization***

---


"""

def book_recommendations(Titles, similarity_data = sim_df, items = data[['ISBN', 'Authors', 'Titles']], k = 5):
    """
    Rekomendasi Resto berdasarkan kemiripan dataframe

    Parameter:
    ---
    Title : tipe data string (str)
                Judul Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan resto sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,Titles].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop Titles agar Judul Buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(Titles, errors = 'ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""***Sample Retrieval***

---


"""

data[data.Titles.eq('Sleep Deep')]

"""***Recommendation Result -- Top-N Recommendation***

---


"""

book_recommendations('Sleep Deep')

"""### ***Model Development - Collaborative Filtering***

---

1. Model Class Initialization
2. Callback Functions Initialization
3. Model Compilation
4. Model Training
5. Sample Retrieval
6. Prediction Result -- Top-N Recommendation

***Model Class Initialization***


---
"""

class RecommenderNet(tf.keras.Model):


  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.books_embedding = layers.Embedding( # layer embeddings books
        num_books,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.books_bias = layers.Embedding(num_books, 1) # layer embedding books bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    books_vector = self.books_embedding(inputs[:, 1]) # memanggil layer embedding 3
    books_bias = self.books_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_books = tf.tensordot(user_vector, books_vector, 2)

    x = dot_user_books + user_bias + books_bias

    return tf.nn.sigmoid(x)

"""***Callback Functions Initialization***

---


"""

# implementing callbacks
from tensorflow.keras.callbacks import ReduceLROnPlateau

lr_reduction = ReduceLROnPlateau(
    monitor = 'root_mean_squared_error',
    patience = 3,
    verbose = 1,
    factor = 0.1,
    min_lr = 0.000001
)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    rmse = logs.get('root_mean_squared_error')
    val_rmse = logs.get('val_root_mean_squared_error')

    if(rmse < 0.2):
      print("\nReached wanted accuracy so cancelling training!")
      self.model.stop_training = True

earlyStop = myCallback()

"""***Model Compilation***

---


"""

# model initialization
model = RecommenderNet(num_users, num_books, 50)

# model compiling
model.compile(
    loss = 'binary_crossentropy',
    optimizer = tf.optimizers.Adam(learning_rate = 0.001),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

"""***Model Training***

---


"""

# model training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 40,
    verbose = 1,
    validation_data = (x_val, y_val),
    callbacks = [lr_reduction, earlyStop]
)

"""***Sample Retrieval***

---


"""

# retrieving sample
user_id = df['User-ID'].sample(1).iloc[0]
rated_books = df[df['User-ID'] == user_id]

unrated_books = df[~df['Book-Title'].isin(rated_books['Book-Title'].values)]['Book-Title']
unrated_books = list(
    set(unrated_books)
    .intersection(set(encode_title1.keys()))
)

unrated_books = [[encode_title1.get(x)] for x in unrated_books]
user_encoder = encode_user_id1.get(user_id)
user_books_array = np.hstack(
    ([[user_encoder]] * len(unrated_books), unrated_books)
)

rated_books

"""***Prediction Result -- Top-N Recommendation***

---


"""

# model prediction
ratings = model.predict(user_books_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_books_ids = [
    encoded_title2.get(unrated_books[x][0]) for x in top_ratings_indices
]

# showing results
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 50)
print('Books with high ratings from user')
print('----' * 50)

top_books_user = (
    rated_books.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    ['Book-Title'].values
)

books_df_rows = df[df['Book-Title'].isin(top_books_user)]
books_df_rows = books_df_rows.rename(columns = {'Book-Title':'Title'})
books_df_rows = books_df_rows.rename(columns = {'Book-Author':'Author'})
for row in books_df_rows.itertuples():
    print(row.Title, ':', row.ISBN, ':', row.Author)

print('----' * 50)
print('Top 10 books recommendation')
print('----' * 50)

recommended_books = df[df['Book-Title'].isin(recommended_books_ids)]
recommended_books = recommended_books.rename(columns = {'Book-Title':'Title'})
recommended_books = recommended_books.rename(columns = {'Book-Author':'Author'})

for row in recommended_books.itertuples():
    print(row.Title, ':', row.ISBN, ':', row.Author)

top_books_user

books_df_rows

"""### ***Evaluation***

---

***Prediction Result - Content Based Filtering***
"""

book_recommendations('Sleep Deep')

"""***Recommedation System Precision***

$$Precision@k = \frac{Rr}{Ra}$$

  dimana,

  - Precision@k = presisi dari k-prediksi rekomendasi
  - Rr = jumlah rekomendasi relevan
  - Ra = jumlah seluruh rekomendasi

---

Hasil dari model Content Based Filtering ini sangat baik, dikarenakan rekomendasi yang dibuat dari model menghasilkan 5/5 rekomendasi relevan dengan input, yaitu buku-buku yang memiliki kata kunci kemiripan dengan input, yaitu 'Sleep' dan 'Deep'

***Metric Visualization -  Collaborative Filtering***
"""

# plot results
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()

"""Pada hasil training dan validasi ini, meskipun pada grafik terlihat RMSE test dan train berbeda jauh, nilainya secara berurutan adalah sebagai berikut:

- root_mean_squared_error: 0.1998
- val_root_mean_squared_error: 0.3531

Ketika dihitung, dapat diketahui bahwa perbedaan RMSE dan validation RMSE hanya ~0.16, sehingga perbedaannya tidak terlalu signifikan. Alhasil, model Collaborative Filtering ini tidak overfit, melainkan sebuah model good fit.

---

### ***Hasil dan Kesimpulan Proyek***

Berdasarkan dari hasil Data Understanding, Data Preparation, Model Development, dan Evaluation, kesimpulan Proyek ini dapat disimpulkan sebagai berikut.

1. Menjawab Problem Statement 1: Dengan banyaknya jumlah buku dan user yang ada, Recommendation System efektif **dapat dibuat dengan baik.**
2. Jalannya proyek ini sudah sesuai dan memuaskan Penulis, dimulai dari menjawab Problem Statement, mencapai Recommendation System Goals yang dirumuskan, serta dapat mengevaluasi performa dengan baik. Alhasil, harapannya proyek ini dapat menjadi sarana bagi sasaran-sasaran yang sudah diuraikan, dan menjadi referensi bagi pelajar-pelajar Machine Learning, terutama pembuat Recommendation System.
"""